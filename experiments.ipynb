{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision \n",
    "import torchvision.transforms as t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONV_KWARGS = {'kernel_size': 3, 'padding': 'same', 'bias': False}\n",
    "\n",
    "BATCHSIZE = 512\n",
    "BIAS_SCALER = 32\n",
    "\n",
    "# To replicate the ~95.77% accuracy in 188 seconds runs, simply change the base_depth from 64->128 and the num_epochs from 10->80\n",
    "HYP = {\n",
    "    'opt': {\n",
    "        'bias_lr':        1.15 * 1.35 * 1. * BIAS_SCALER/BATCHSIZE, # TODO: How we're expressing this information feels somewhat clunky, is there maybe a better way to do this? :'))))\n",
    "        'non_bias_lr':    1.15 * 1.35 * 1. / BATCHSIZE,\n",
    "        'bias_decay':     .85 * 4.8e-4 * BATCHSIZE/BIAS_SCALER,\n",
    "        'non_bias_decay': .85 * 4.8e-4 * BATCHSIZE,\n",
    "        'scaling_factor': 1./10,\n",
    "        'percent_start': .2,\n",
    "    },\n",
    "    'net': {\n",
    "        'whitening': {\n",
    "            'kernel_size': 2,\n",
    "            'num_examples': 50000,\n",
    "        },\n",
    "        'batch_norm_momentum': .8,\n",
    "        'cutout_size': 0,\n",
    "        'pad_amount': 3,\n",
    "        'base_depth': 64 ## This should be a factor of 8 in some way to stay tensor core friendly\n",
    "    },\n",
    "    'misc': {\n",
    "        'ema': {\n",
    "            'epochs': 2,\n",
    "            'decay_base': .986,\n",
    "            'every_n_steps': 2,\n",
    "        },\n",
    "        'train_epochs': 10,\n",
    "        'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'data_location': 'data.pt',\n",
    "    }\n",
    "}\n",
    "\n",
    "SCALER = 2. ## You can play with this on your own if you want, for the first beta I wanted to keep things simple (for now) and leave it out of the hyperparams dict\n",
    "DEPTHS = {\n",
    "    'init':   round(SCALER**-1*HYP['net']['base_depth']), # 64  w/ scaler at base value\n",
    "    'block1': round(SCALER**1*HYP['net']['base_depth']), # 128 w/ scaler at base value\n",
    "    'block2': round(SCALER**2*HYP['net']['base_depth']), # 256 w/ scaler at base value\n",
    "    'block3': round(SCALER**3*HYP['net']['base_depth']), # 512 w/ scaler at base value\n",
    "    'num_classes': 10\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Tensor is not a torch image.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     26\u001b[0m train_dataset_gpu_loader \u001b[39m=\u001b[39m DataLoader(cifar10, batch_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(cifar10), drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, persistent_workers\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m eval_dataset_gpu_loader \u001b[39m=\u001b[39m DataLoader(cifar10_eval, batch_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(cifar10_eval), drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, persistent_workers\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 31\u001b[0m train_dataset_gpu \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m([item\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mHYP[\u001b[39m'\u001b[39m\u001b[39mmisc\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m], non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_dataset_gpu_loader))])\n\u001b[1;32m     32\u001b[0m eval_dataset_gpu  \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m([item\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mHYP[\u001b[39m'\u001b[39m\u001b[39mmisc\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m], non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(eval_dataset_gpu_loader))])\n\u001b[1;32m     34\u001b[0m data \u001b[39m=\u001b[39m {\n\u001b[1;32m     35\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: train_dataset_gpu,\n\u001b[1;32m     36\u001b[0m     \u001b[39m'\u001b[39m\u001b[39meval\u001b[39m\u001b[39m'\u001b[39m: eval_dataset_gpu\n\u001b[1;32m     37\u001b[0m }\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:451\u001b[0m, in \u001b[0;36mPad.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    444\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be padded.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m        PIL Image or Tensor: Padded image.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mpad(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfill, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_mode)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:525\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(img, padding, fill, padding_mode)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    523\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39mpad(img, padding\u001b[39m=\u001b[39mpadding, fill\u001b[39m=\u001b[39mfill, padding_mode\u001b[39m=\u001b[39mpadding_mode)\n\u001b[0;32m--> 525\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mpad(img, padding\u001b[39m=\u001b[39;49mpadding, fill\u001b[39m=\u001b[39;49mfill, padding_mode\u001b[39m=\u001b[39;49mpadding_mode)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:384\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(img, padding, fill, padding_mode)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpad\u001b[39m(\n\u001b[1;32m    382\u001b[0m     img: Tensor, padding: Union[\u001b[39mint\u001b[39m, List[\u001b[39mint\u001b[39m]], fill: Optional[Union[\u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, padding_mode: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 384\u001b[0m     _assert_image_tensor(img)\n\u001b[1;32m    386\u001b[0m     \u001b[39mif\u001b[39;00m fill \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m         fill \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:15\u001b[0m, in \u001b[0;36m_assert_image_tensor\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_image_tensor\u001b[39m(img: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_tensor_a_torch_image(img):\n\u001b[0;32m---> 15\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTensor is not a torch image.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Tensor is not a torch image."
     ]
    }
   ],
   "source": [
    "if not os.path.exists(HYP['misc']['data_location']):\n",
    "\n",
    "        CIFAR10_MEAN, CIFAR10_STD = [\n",
    "            torch.tensor([0.4913997551666284, 0.48215855929893703,\n",
    "                         0.4465309133731618], device=HYP['misc']['device']),\n",
    "            torch.tensor([0.24703225141799082, 0.24348516474564,\n",
    "                         0.26158783926049628],  device=HYP['misc']['device'])\n",
    "        ]\n",
    "\n",
    "        PREP_TRANSFORM = t.Compose([TO_TENSOR := t.ToTensor(),\n",
    "                                    NORMALIZE := t.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "                                    FLATTEN := t.Lambda(lambda x:x.view(-1)),\n",
    "                                    PAD := t.Pad(BORDER := 4)])\n",
    "\n",
    "        TRAIN_TRANSFORM = t.Compose([PREP_TRANSFORM,\n",
    "                                     CROP := t.CenterCrop(IMAGE_SIZE := (32, 32)),\n",
    "                                     FLIP := t.RandomVerticalFlip()\n",
    "                                     ])\n",
    "\n",
    "        cifar10 = torchvision.datasets.CIFAR10(\n",
    "            'cifar10/', download=True,  train=True,  transform=TRAIN_TRANSFORM)\n",
    "        cifar10_eval = torchvision.datasets.CIFAR10(\n",
    "            'cifar10/', download=False, train=False, transform=PREP_TRANSFORM)\n",
    "\n",
    "        # use   the dataloader to get a single batch of all of the dataset items at once.\n",
    "        train_dataset_gpu_loader = DataLoader(cifar10, batch_size=len(cifar10), drop_last=True, persistent_workers=False)\n",
    "\n",
    "        eval_dataset_gpu_loader = DataLoader(cifar10_eval, batch_size=len(cifar10_eval), drop_last=True, persistent_workers=False)\n",
    "\n",
    "                                                 \n",
    "        train_dataset_gpu = zip([item.to(device=HYP['misc']['device'], non_blocking=True) for item in next(iter(train_dataset_gpu_loader))])\n",
    "        eval_dataset_gpu  = zip([item.to(device=HYP['misc']['device'], non_blocking=True) for item in next(iter(eval_dataset_gpu_loader))])\n",
    "\n",
    "        data = {\n",
    "            'train': train_dataset_gpu,\n",
    "            'eval': eval_dataset_gpu\n",
    "        }\n",
    "\n",
    "        torch.save(data, HYP['misc']['data_location'])\n",
    "\n",
    "else:\n",
    "    ## This is effectively instantaneous, and takes us practically straight to where the dataloader-loaded dataset would be. :)\n",
    "    ## So as long as you run the above loading process once, and keep the file on the disc it's specified by default in the above\n",
    "    ## HYP dictionary, then we should be good. :)\n",
    "    data = torch.load(HYP['misc']['data_location'])\n",
    "\n",
    "\n",
    "## As you'll note above and below, one difference is that we don't count loading the raw data to GPU since it's such a variable operation, and can sort of get in the way\n",
    "## of measuring other things. That said, measuring the preprocessing (outside of the PADding) is still important to us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_like(weights):\n",
    "    return [torch.zeros_like(w) for w in weights]\n",
    "\n",
    "def nesterov_update(w, dw, v, lr, weight_decay, momentum):\n",
    "    dw.add_(weight_decay, w).mul_(-lr)\n",
    "    v.mul_(momentum).add_(dw)\n",
    "    w.add_(dw.add_(momentum, v))\n",
    "\n",
    "def optimiser(weights, param_schedule, update, state_init):\n",
    "    weights = list(weights)\n",
    "    return {'update': update, 'param_schedule': param_schedule, 'step_number': 0, 'weights': weights,  'opt_state': state_init(weights)}\n",
    "\n",
    "SGD = partial(optimiser, update=nesterov_update, state_init=zeros_like)\n",
    "\n",
    "def train(model, lr_schedule, train_set, test_set, batch_size, num_workers=0):\n",
    "    train_batches = DataLoader(train_set, batch_size, shuffle=True, set_random_choices=True, num_workers=num_workers)\n",
    "    test_batches = DataLoader(test_set, batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    lr = lambda step: lr_schedule(step/len(train_batches))/batch_size\n",
    "    opts = [SGD(trainable_params(model).values(), {'lr': lr, 'weight_decay': (5e-4*batch_size), 'momentum': 0.9})]\n",
    "    logs, state = Table(), {MODEL: model, LOSS: x_ent_loss, OPTS: opts}\n",
    "    for epoch in range(lr_schedule.knots[-1]):\n",
    "        logs.append({**{'epoch': epoch+1, 'lr': lr_schedule(epoch+1)}, \n",
    "                          **train_epoch(state, Timer(torch.cuda.synchronize), train_batches, test_batches)})\n",
    "    return logs\n",
    "\n",
    "def train_epoch(state, timer, train_batches, valid_batches, train_steps=default_train_steps, valid_steps=default_valid_steps, \n",
    "            on_epoch_end=(lambda state: state)):\n",
    "    train_summary, train_time = epoch_stats(on_epoch_end(reduce(train_batches, state, train_steps))), timer()\n",
    "    valid_summary, valid_time = epoch_stats(reduce(valid_batches, state, valid_steps)), timer(include_in_total=False) #DAWNBench rules\n",
    "    return {\n",
    "        'train': {**{'time': train_time}, **train_summary}, \n",
    "        'valid': {**{'time': valid_time}, **valid_summary}, \n",
    "        'total time': timer.total_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat(nn.Module):\n",
    "    def __init__(self, modules: OrderedDict[str, nn.Module]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        for name, module in modules.items():\n",
    "            setattr(Cat, name, module)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return torch.cat([module(x) for _, module in self.modules()]) # type: ignore\n",
    "\n",
    "class Add(nn.Module):\n",
    "    def __init__(self, modules: OrderedDict[str, nn.Module]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        for name, module in modules.items():\n",
    "            setattr(Cat, name, module)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return sum([module(x) for _, module in self.modules()]) # type: ignore\n",
    "\n",
    "class Id(nn.Module):\n",
    "    def forward(self, x: Tensor): return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x: Tensor): return x.view(x.size(0), x.size(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, weight_requires_grad=True, bias_requires_grad=True, weights_init=False, *args, **kwargs):\n",
    "\n",
    "        super().__init__(num_features, *args, **kwargs)\n",
    "\n",
    "        if weights_init:\n",
    "            self.weight.data.fill_(1.0)\n",
    "            self.bias.data.fill_(0.0)\n",
    "\n",
    "        self.weight.requires_grad=weight_requires_grad\n",
    "        self.bias.requires_grad=bias_requires_grad\n",
    "\n",
    "# Allows us to set default arguments for the whole convolution itself.\n",
    "class Conv(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        kwargs = {**kwargs, **DEFAULT_CONV_KWARGS}\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Sequential):\n",
    "    def __init__(self, c_in: int, c_out: int, stride: int=1) -> None:\n",
    "\n",
    "        bn1 = BatchNorm(c_in)\n",
    "        relu1 = nn.ReLU(inplace = True)\n",
    "\n",
    "        branch = nn.Sequential(OrderedDict([\n",
    "            ('conv1', Conv(c_in, c_out, kernel_size=3,\n",
    "                            stride=stride, padding=1)),\n",
    "            ('bn2', BatchNorm(c_out)),\n",
    "            ('relu2', nn.ReLU(inplace=True)),\n",
    "            ('conv2', Conv(c_out, c_out, bias=False)),]))\n",
    "\n",
    "        super().__init__(\n",
    "        bn1,\n",
    "        relu1,\n",
    "        Add(OrderedDict([\n",
    "            (('conv3', Conv(\n",
    "                c_in, c_out, kernel_size=1, stride=stride, padding=0, bias=False)) if (projection := (stride != 1) or (c_in != c_out)) else (\"id\", Id())),\n",
    "            (\"branch\", branch)\n",
    "        ])),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DawnNet(nn.Sequential):\n",
    "\n",
    "    def __init__(self, c = 64, Block = ResBlock, prep_bn_relu = False, concat_pool = True, **kw) -> None:\n",
    "\n",
    "        if isinstance(c, int):\n",
    "            c=[c, 2*c, 4*c, 4*c]\n",
    "\n",
    "        classifier_pool = Cat(OrderedDict([('maxpool', nn.MaxPool2d(4)),\n",
    "                                           (('avgpool', (nn.AvgPool2d(4), ['in'])) if concat_pool else ('pool', nn.MaxPool2d(4)))\n",
    "                                           ]))\n",
    "\n",
    "        super().__init__(OrderedDict([\n",
    "            ('input', (None, [])),\n",
    "            ('prep', nn.Sequential(OrderedDict([\n",
    "                ('conv', Conv(3, c[0], bias=False)),\n",
    "                ('bn', BatchNorm(c[0], **kw)),\n",
    "                ('relu', nn.ReLU(True) if prep_bn_relu else None)\n",
    "            ]))),\n",
    "            ('layer1', nn.Sequential(OrderedDict([\n",
    "                ('block0', Block(c[0], c[0], **kw)),\n",
    "                ('block1', Block(c[0], c[0], **kw))\n",
    "            ]))),\n",
    "            ('layer2', nn.Sequential(OrderedDict([\n",
    "                ('block0', Block(c[0], c[1], stride=2, **kw)),\n",
    "                ('block1', Block(c[1], c[1], **kw))\n",
    "            ]))),\n",
    "            ('layer3', nn.Sequential(OrderedDict([\n",
    "                ('block0', Block(c[1], c[2], stride=2, **kw)),\n",
    "                ('block1', Block(c[2], c[2], **kw))\n",
    "            ]))),\n",
    "            ('layer4', nn.Sequential(OrderedDict([\n",
    "                ('block0', Block(c[2], c[3], stride=2, **kw)),\n",
    "                ('block1', Block(c[3], c[3], **kw))\n",
    "            ]))),\n",
    "            ('final', nn.Sequential(OrderedDict([\n",
    "                ('pool', classifier_pool),\n",
    "                ('flatten', Flatten()),\n",
    "                ('linear', nn.Linear((2*c[3] if concat_pool else c[3]), 10, bias=True))\n",
    "            ]))),\n",
    "            ('logits', Id()),\n",
    "            ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "padding='same' is not supported for strided convolutions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net \u001b[39m=\u001b[39m DawnNet()\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(net)\n",
      "Cell \u001b[0;32mIn[7], line 50\u001b[0m, in \u001b[0;36mDawnNet.__init__\u001b[0;34m(self, c, Block, prep_bn_relu, concat_pool, **kw)\u001b[0m\n\u001b[1;32m     32\u001b[0m     c\u001b[39m=\u001b[39m[c, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mc, \u001b[39m4\u001b[39m\u001b[39m*\u001b[39mc, \u001b[39m4\u001b[39m\u001b[39m*\u001b[39mc]\n\u001b[1;32m     34\u001b[0m classifier_pool \u001b[39m=\u001b[39m Cat(OrderedDict([(\u001b[39m'\u001b[39m\u001b[39mmaxpool\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mMaxPool2d(\u001b[39m4\u001b[39m)),\n\u001b[1;32m     35\u001b[0m                                    ((\u001b[39m'\u001b[39m\u001b[39mavgpool\u001b[39m\u001b[39m'\u001b[39m, (nn\u001b[39m.\u001b[39mAvgPool2d(\u001b[39m4\u001b[39m), [\u001b[39m'\u001b[39m\u001b[39min\u001b[39m\u001b[39m'\u001b[39m])) \u001b[39mif\u001b[39;00m concat_pool \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mpool\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mMaxPool2d(\u001b[39m4\u001b[39m)))\n\u001b[1;32m     36\u001b[0m                                    ]))\n\u001b[1;32m     38\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(OrderedDict([\n\u001b[1;32m     39\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m, (\u001b[39mNone\u001b[39;00m, [])),\n\u001b[1;32m     40\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mprep\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mSequential(OrderedDict([\n\u001b[1;32m     41\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mconv\u001b[39m\u001b[39m'\u001b[39m, Conv(\u001b[39m3\u001b[39m, c[\u001b[39m0\u001b[39m], bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)),\n\u001b[1;32m     42\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mbn\u001b[39m\u001b[39m'\u001b[39m, BatchNorm(c[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)),\n\u001b[1;32m     43\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mReLU(\u001b[39mTrue\u001b[39;00m) \u001b[39mif\u001b[39;00m prep_bn_relu \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m     ]))),\n\u001b[1;32m     45\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mlayer1\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mSequential(OrderedDict([\n\u001b[1;32m     46\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mblock0\u001b[39m\u001b[39m'\u001b[39m, Block(c[\u001b[39m0\u001b[39m], c[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)),\n\u001b[1;32m     47\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mblock1\u001b[39m\u001b[39m'\u001b[39m, Block(c[\u001b[39m0\u001b[39m], c[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw))\n\u001b[1;32m     48\u001b[0m     ]))),\n\u001b[1;32m     49\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mlayer2\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mSequential(OrderedDict([\n\u001b[0;32m---> 50\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mblock0\u001b[39m\u001b[39m'\u001b[39m, Block(c[\u001b[39m0\u001b[39;49m], c[\u001b[39m1\u001b[39;49m], stride\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)),\n\u001b[1;32m     51\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mblock1\u001b[39m\u001b[39m'\u001b[39m, Block(c[\u001b[39m1\u001b[39m], c[\u001b[39m1\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw))\n\u001b[1;32m     52\u001b[0m     ]))),\n\u001b[1;32m     53\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mlayer3\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mSequential(OrderedDict([\n\u001b[1;32m     54\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mblock0\u001b[39m\u001b[39m'\u001b[39m, Block(c[\u001b[39m1\u001b[39m], c[\u001b[39m2\u001b[39m], stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)),\n\u001b[1;32m     55\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mblock1\u001b[39m\u001b[39m'\u001b[39m, Block(c[\u001b[39m2\u001b[39m], c[\u001b[39m2\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw))\n\u001b[1;32m     56\u001b[0m     ]))),\n\u001b[1;32m     57\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mlayer4\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mSequential(OrderedDict([\n\u001b[1;32m     58\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mblock0\u001b[39m\u001b[39m'\u001b[39m, Block(c[\u001b[39m2\u001b[39m], c[\u001b[39m3\u001b[39m], stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)),\n\u001b[1;32m     59\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mblock1\u001b[39m\u001b[39m'\u001b[39m, Block(c[\u001b[39m3\u001b[39m], c[\u001b[39m3\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw))\n\u001b[1;32m     60\u001b[0m     ]))),\n\u001b[1;32m     61\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mfinal\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mSequential(OrderedDict([\n\u001b[1;32m     62\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mpool\u001b[39m\u001b[39m'\u001b[39m, classifier_pool),\n\u001b[1;32m     63\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mflatten\u001b[39m\u001b[39m'\u001b[39m, Flatten()),\n\u001b[1;32m     64\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mLinear((\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mc[\u001b[39m3\u001b[39m] \u001b[39mif\u001b[39;00m concat_pool \u001b[39melse\u001b[39;00m c[\u001b[39m3\u001b[39m]), \u001b[39m10\u001b[39m, bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m     65\u001b[0m     ]))),\n\u001b[1;32m     66\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m, Id()),\n\u001b[1;32m     67\u001b[0m     ]))\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mResBlock.__init__\u001b[0;34m(self, c_in, c_out, stride)\u001b[0m\n\u001b[1;32m      4\u001b[0m bn1 \u001b[39m=\u001b[39m BatchNorm(c_in)\n\u001b[1;32m      5\u001b[0m relu1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mReLU(inplace \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m branch \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(OrderedDict([\n\u001b[0;32m----> 8\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mconv1\u001b[39m\u001b[39m'\u001b[39m, Conv(c_in, c_out, kernel_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m                     stride\u001b[39m=\u001b[39;49mstride, padding\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)),\n\u001b[1;32m     10\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mbn2\u001b[39m\u001b[39m'\u001b[39m, BatchNorm(c_out)),\n\u001b[1;32m     11\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mrelu2\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mReLU(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)),\n\u001b[1;32m     12\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mconv2\u001b[39m\u001b[39m'\u001b[39m, Conv(c_out, c_out, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)),]))\n\u001b[1;32m     14\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m     15\u001b[0m bn1,\n\u001b[1;32m     16\u001b[0m relu1,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m ])),\n\u001b[1;32m     22\u001b[0m )\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mConv.__init__\u001b[0;34m(self, in_channels, out_channels, *args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, in_channels, out_channels, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     16\u001b[0m     kwargs \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mDEFAULT_CONV_KWARGS}\n\u001b[0;32m---> 17\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(in_channels, out_channels, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:450\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    448\u001b[0m padding_ \u001b[39m=\u001b[39m padding \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(padding, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m _pair(padding)\n\u001b[1;32m    449\u001b[0m dilation_ \u001b[39m=\u001b[39m _pair(dilation)\n\u001b[0;32m--> 450\u001b[0m \u001b[39msuper\u001b[39;49m(Conv2d, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    451\u001b[0m     in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n\u001b[1;32m    452\u001b[0m     \u001b[39mFalse\u001b[39;49;00m, _pair(\u001b[39m0\u001b[39;49m), groups, bias, padding_mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:100\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mInvalid padding string \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m, should be one of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     98\u001b[0m                 padding, valid_padding_strings))\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m padding \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(s \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m stride):\n\u001b[0;32m--> 100\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpadding=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not supported for strided convolutions\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m valid_padding_modes \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreflect\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreplicate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcircular\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m padding_mode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m valid_padding_modes:\n",
      "\u001b[0;31mValueError\u001b[0m: padding='same' is not supported for strided convolutions"
     ]
    }
   ],
   "source": [
    "net = DawnNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can hack any changes to each residual group that you want directly in here\n",
    "class ConvGroup(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, residual, short, pool, se):\n",
    "        super().__init__()\n",
    "        self.short = short\n",
    "        self.pool = pool\n",
    "        self.se = se\n",
    "\n",
    "        self.residual = residual\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.conv1 = Conv(channels_in, channels_out)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.norm1 = BatchNorm(channels_out)\n",
    "        self.activ = nn.GELU()          \n",
    "\n",
    "        if not short:\n",
    "            self.conv2 = Conv(channels_out, channels_out)\n",
    "            self.conv3 = Conv(channels_out, channels_out)\n",
    "            self.norm2 = BatchNorm(channels_out)\n",
    "            self.norm3 = BatchNorm(channels_out)\n",
    "\n",
    "            self.se1 = nn.Linear(channels_out, channels_out//16)\n",
    "            self.se2 = nn.Linear(channels_out//16, channels_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.pool:\n",
    "            x = self.pool1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.activ(x)\n",
    "        if self.short: # layer 2 doesn't necessarily need the residual, so we just return it.\n",
    "            return x\n",
    "        residual = x\n",
    "        if self.se:\n",
    "            mult = torch.sigmoid(self.se2(self.activ(self.se1(torch.mean(residual, dim=(2,3)))))).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.activ(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        if self.se:\n",
    "            x = x * mult\n",
    "\n",
    "        x = self.norm3(x)\n",
    "        x = self.activ(x)\n",
    "        x = x + residual # haiku\n",
    "\n",
    "        return x\n",
    "\n",
    "# Set to 1 for now just to debug a few things....\n",
    "class TemperatureScaler(nn.Module):\n",
    "    def __init__(self, init_val):\n",
    "        super().__init__()\n",
    "        self.scaler = torch.tensor(init_val)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.float() ## save precision for the gradients in the backwards pass\n",
    "                  ## I personally believe from experience that this is important\n",
    "                  ## for a few reasons. I believe this is the main functional difference between\n",
    "                  ## my implementation, and David's implementation...\n",
    "        return x.mul(self.scaler)\n",
    "\n",
    "class FastGlobalMaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Previously was chained torch.max calls.\n",
    "        # requires less time than AdaptiveMax2dPooling -- about ~.3s for the entire run, in fact (which is pretty significant! :O :D :O :O <3 <3 <3 <3)\n",
    "        return torch.amax(x, dim=(2,3)) # Global maximum pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net():\n",
    "    # TODO: A way to make this cleaner??\n",
    "    # Note, you have to specify any arguments overlapping with defaults (i.e. everything but in/out depths) as kwargs so that they are properly overridden (TODO cleanup somehow?)\n",
    "    whiten_conv_depth = 3*HYP['net']['whitening']['kernel_size']**2\n",
    "    network_dict = nn.ModuleDict({\n",
    "        'initial_block': nn.ModuleDict({\n",
    "            'whiten': Conv(3, whiten_conv_depth, kernel_size=HYP['net']['whitening']['kernel_size'], padding=0),\n",
    "            'project': Conv(whiten_conv_depth, DEPTHS['init'], kernel_size=1),\n",
    "            'norm': BatchNorm(DEPTHS['init'], weight=False),\n",
    "            'activation': nn.GELU(),\n",
    "        }),\n",
    "        'residual1': ConvGroup(DEPTHS['init'], DEPTHS['block1'], residual=True, short=False, pool=True, se=True),\n",
    "        'residual2': ConvGroup(DEPTHS['block1'], DEPTHS['block2'], residual=True, short=True, pool=True, se=True),\n",
    "        'residual3': ConvGroup(DEPTHS['block2'], DEPTHS['block3'], residual=True, short=False, pool=True, se=True),\n",
    "        'pooling': FastGlobalMaxPooling(),\n",
    "        'linear': nn.Linear(DEPTHS['block3'], DEPTHS['num_classes'], bias=False),\n",
    "        'temperature': TemperatureScaler(HYP['opt']['scaling_factor'])\n",
    "    })\n",
    "\n",
    "    net = SpeedyResNet(network_dict)\n",
    "    net = net.to(HYP['misc']['device'])\n",
    "    net = net.to(memory_format=torch.channels_last) # to appropriately use tensor cores/avoid thrash while training\n",
    "    net.train()\n",
    "    net.half() # Convert network to half before initializing the initial whitening layer.\n",
    "\n",
    "    ## Initialize the whitening convolution\n",
    "    with torch.no_grad():\n",
    "        # Initialize the first layer to be fixed weights that whiten the expected input values of the network be on the unit HYPersphere. (i.e. their...average vector length is 1.?, IIRC)\n",
    "        init_whitening_conv(net.net_dict['initial_block']['whiten'],\n",
    "                            data['train']['images'].index_select(0, torch.randperm(data['train']['images'].shape[0], device=data['train']['images'].device)),\n",
    "                            num_examples=HYP['net']['whitening']['num_examples'],\n",
    "                            pad_amount=HYP['net']['pad_amount'],\n",
    "                            whiten_splits=5000) ## Hardcoded for now while we figure out the optimal whitening number\n",
    "                                                ## If you're running out of memory (OOM) feel free to decrease this, but\n",
    "                                                ## the index lookup in the dataloader may give you some trouble depending\n",
    "                                                ## upon exactly how memory-limited you are\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeedyResNet(nn.Module):\n",
    "    def __init__(self, network_dict):\n",
    "        super().__init__()\n",
    "        self.net_dict = network_dict # flexible, defined in the make_net function\n",
    "\n",
    "    # This allows you to customize/change the execution order of the network as needed.\n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            x = torch.cat((x, torch.flip(x, (-1,))))\n",
    "        x = self.net_dict['initial_block']['whiten'](x)\n",
    "        x = self.net_dict['initial_block']['project'](x)\n",
    "        x = self.net_dict['initial_block']['norm'](x)\n",
    "        x = self.net_dict['initial_block']['activation'](x)\n",
    "        x = self.net_dict['residual1'](x)\n",
    "        x = self.net_dict['residual2'](x)\n",
    "        x = self.net_dict['residual3'](x)\n",
    "        x = self.net_dict['pooling'](x)\n",
    "        x = self.net_dict['linear'](x)\n",
    "        x = self.net_dict['temperature'](x)\n",
    "        if not self.training:\n",
    "            # Average the predictions from the lr-flipped inputs during eval\n",
    "            orig, flipped = x.split(x.shape[0]//2, dim=0)\n",
    "            x = .5 * orig + .5 * flipped\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = 2. ## You can play with this on your own if you want, for the first beta I wanted to keep things simple (for now) and leave it out of the hyperparams dict\n",
    "depths = {\n",
    "    'init':   round(scaler**-1*hyp['net']['base_depth']), # 64  w/ scaler at base value\n",
    "    'block1': round(scaler**1*hyp['net']['base_depth']), # 128 w/ scaler at base value\n",
    "    'block2': round(scaler**2*hyp['net']['base_depth']), # 256 w/ scaler at base value\n",
    "    'block3': round(scaler**3*hyp['net']['base_depth']), # 512 w/ scaler at base value\n",
    "    'num_classes': 10\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def make_net():\n",
    "    # TODO: A way to make this cleaner??\n",
    "    # Note, you have to specify any arguments overlapping with defaults (i.e. everything but in/out depths) as kwargs so that they are properly overridden (TODO cleanup somehow?)\n",
    "    whiten_conv_depth = 3*hyp['net']['whitening']['kernel_size']**2\n",
    "    network_dict = nn.ModuleDict({\n",
    "        'initial_block': nn.ModuleDict({\n",
    "            'whiten': Conv(3, whiten_conv_depth, kernel_size=hyp['net']['whitening']['kernel_size'], padding=0),\n",
    "            'project': Conv(whiten_conv_depth, depths['init'], kernel_size=1),\n",
    "            'norm': BatchNorm(depths['init'], weight=False),\n",
    "            'activation': nn.GELU(),\n",
    "        }),\n",
    "        'residual1': ConvGroup(depths['init'], depths['block1'], residual=True, short=False, pool=True, se=True),\n",
    "        'residual2': ConvGroup(depths['block1'], depths['block2'], residual=True, short=True, pool=True, se=True),\n",
    "        'residual3': ConvGroup(depths['block2'], depths['block3'], residual=True, short=False, pool=True, se=True),\n",
    "        'pooling': FastGlobalMaxPooling(),\n",
    "        'linear': nn.Linear(depths['block3'], depths['num_classes'], bias=False),\n",
    "        'temperature': TemperatureScaler(hyp['opt']['scaling_factor'])\n",
    "    })\n",
    "\n",
    "    net = SpeedyResNet(network_dict)\n",
    "    net = net.to(hyp['misc']['device'])\n",
    "    net = net.to(memory_format=torch.channels_last) # to appropriately use tensor cores/avoid thrash while training\n",
    "    net.train()\n",
    "    net.half() # Convert network to half before initializing the initial whitening layer.\n",
    "\n",
    "    ## Initialize the whitening convolution\n",
    "    with torch.no_grad():\n",
    "        # Initialize the first layer to be fixed weights that whiten the expected input values of the network be on the unit hypersphere. (i.e. their...average vector length is 1.?, IIRC)\n",
    "        init_whitening_conv(net.net_dict['initial_block']['whiten'],\n",
    "                            data['train']['images'].index_select(0, torch.randperm(data['train']['images'].shape[0], device=data['train']['images'].device)),\n",
    "                            num_examples=hyp['net']['whitening']['num_examples'],\n",
    "                            pad_amount=hyp['net']['pad_amount'],\n",
    "                            whiten_splits=5000) ## Hardcoded for now while we figure out the optimal whitening number\n",
    "                                                ## If you're running out of memory (OOM) feel free to decrease this, but\n",
    "                                                ## the index lookup in the dataloader may give you some trouble depending\n",
    "                                                ## upon exactly how memory-limited you are\n",
    "\n",
    "    return net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
